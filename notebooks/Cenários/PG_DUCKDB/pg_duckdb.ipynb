{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cee83fc-6f89-47aa-a3b9-0047e6402200",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cae8165-c7f8-4ba3-8c52-5dc0138d1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import os\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cccc98-cd45-47d5-9d89-b3dbc5a4333b",
   "metadata": {},
   "source": [
    "# Sessão SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ec1fd2-98d3-4f9c-9faa-bb31266a7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .config(\"spark.jars.packages\", \n",
    "                    \"org.apache.hadoop:hadoop-aws:3.2.2,\"\n",
    "                    \"io.delta:delta-spark_2.12:3.2.0,\"\n",
    "                    \"io.delta:delta-storage:3.2.0,\"\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.12.180\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.cores\", \"2\") \\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "            .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "            .config(\"spark.dynamicAllocation.maxExecutors\", \"2\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "\t        .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"MINIO_HOST\")) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"MINIO_ACCESS_KEY\")) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"MINIO_SECRET_KEY\")) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"spark.hadoop.fs.AbstractFileSystem.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3A\") \\\n",
    "            .getOrCreate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c656fe-5ce3-47f6-b654-1afddac02b3a",
   "metadata": {},
   "source": [
    "# Tabela Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa9b685-6330-4667-a180-38da189d58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lendo tabela\n",
    "emp_path = \"s3a://bronze/employe\"\n",
    "spark.read.format(\"delta\").load(emp_path).createOrReplaceTempView(\"employe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2de547-9745-4fb3-88e0-d85f56037f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+\n",
      "|emp_id|emp_name|dept_code| salary|\n",
      "+------+--------+---------+-------+\n",
      "|  1002|     Bob|     D102|60000.0|\n",
      "|  1003| Charlie|     D103|70000.0|\n",
      "|  1007|   Grace|     D107|75000.0|\n",
      "|  1006|   Frank|     D106|70000.0|\n",
      "|  1002|     Bob|     D102|62000.0|\n",
      "+------+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from employe\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daac0378-9a89-4709-b5cc-f41ffc62c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO employe (emp_id, emp_name, dept_code, salary)\n",
    "VALUES\n",
    "    (1001, 'Alice', 'D101', 50000),\n",
    "    (1002, 'Bob', 'D102', 60000),\n",
    "    (1003, 'Charlie', 'D103', 70000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2da9cf-8a3e-480e-8926-9159a389a346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DELETE FROM employe WHERE emp_id = '1001'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b7112-a20e-4aa6-9ede-46115f697cd3",
   "metadata": {},
   "source": [
    "# Tabela Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1903c802-58c4-4f86-81fe-d3863101ca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_path = \"s3a://silver/employe_scd2\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employe_scd2 (\n",
    "    emp_id INT,\n",
    "    emp_name STRING,\n",
    "    dept_code STRING,\n",
    "    salary DOUBLE,\n",
    "    start_date DATE,\n",
    "    end_date DATE,\n",
    "    is_current BOOLEAN\n",
    ") USING DELTA LOCATION '{emp_path}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO employe_scd2 (emp_id, emp_name, dept_code, salary, start_date, end_date, is_current)\n",
    "VALUES\n",
    "    (1001, 'Alice', 'D101', 50000, '2020-01-01', NULL, TRUE),\n",
    "    (1002, 'Bob', 'D102', 60000, '2020-01-01', NULL, TRUE),\n",
    "    (1003, 'Charlie', 'D103', 70000, '2020-01-01', NULL, TRUE)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554040d6-53bf-4db7-9a59-d842fcd35f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+----------+--------+----------+\n",
      "|emp_id|emp_name|dept_code| salary|start_date|end_date|is_current|\n",
      "+------+--------+---------+-------+----------+--------+----------+\n",
      "|  1002|     Bob|     D102|60000.0|2020-01-01|    NULL|      true|\n",
      "|  1003| Charlie|     D103|70000.0|2020-01-01|    NULL|      true|\n",
      "|  1001|   Alice|     D101|50000.0|2020-01-01|    NULL|      true|\n",
      "+------+--------+---------+-------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employe_scd2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6709318-5f2b-406f-9e64-ed32f4fa6e45",
   "metadata": {},
   "source": [
    "# Merge SCD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afff46f5-2306-4a9e-b400-2640e98ef569",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeTable  = DeltaTable.forPath(spark, emp_path)  # Tabela Delta que contém os registros de empregados históricos\n",
    "updatesDF  = spark.table(\"employe\")  # DataFrame com os novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b780cbe-cc0e-4e74-a1b2-8c821bb52a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gera Dataframe somente com os registros atualizados\n",
    "newRecordsToInsert = updatesDF.alias(\"updates\") \\\n",
    "    .join(employeTable.toDF().alias(\"employe\"), \"emp_id\") \\\n",
    "    .where(\n",
    "        # Filtra registros atuais e verifica se houve alteração em algum campo\n",
    "        \"(employe.is_current = TRUE AND (updates.salary <> employe.salary OR updates.emp_name <> employe.emp_name OR updates.dept_code <> employe.dept_code))\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c801cb3-1211-4af8-9f12-ca84eb68f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+--------+---------+-------+----------+--------+----------+\n",
      "|emp_id|emp_name|dept_code| salary|emp_name|dept_code| salary|start_date|end_date|is_current|\n",
      "+------+--------+---------+-------+--------+---------+-------+----------+--------+----------+\n",
      "|  1001|   Alice|     D101|58000.0|   Alice|     D101|50000.0|2020-01-01|    NULL|      true|\n",
      "|  1002|     Bob|     D102|62000.0|     Bob|     D102|60000.0|2020-01-01|    NULL|      true|\n",
      "+------+--------+---------+-------+--------+---------+-------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newRecordsToInsert.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d564d173-b21f-4ee6-9258-4be720639ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o DataFrame com os registros a serem inseridos e atualizados\n",
    "stagedUpdates = (\n",
    "    newRecordsToInsert\n",
    "    .selectExpr(\"NULL as mergeKey\", \"updates.*\")  # Cria um mergeKey fictício (NULL) para identificar registros a serem inseridos\n",
    "    .union(updatesDF.selectExpr(\"emp_id as mergeKey\", \"*\"))  # Prepara os registros de updates com mergeKey para serem usados na atualização ou inserção\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c63f4f-0984-4bf4-a130-39c0421ec43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+---------+-------+\n",
      "|mergeKey|emp_id|emp_name|dept_code| salary|\n",
      "+--------+------+--------+---------+-------+\n",
      "|    NULL|  1001|   Alice|     D101|58000.0|\n",
      "|    NULL|  1002|     Bob|     D102|62000.0|\n",
      "|    1007|  1007|   Grace|     D107|75000.0|\n",
      "|    1006|  1006|   Frank|     D106|70000.0|\n",
      "|    1001|  1001|   Alice|     D101|58000.0|\n",
      "|    1002|  1002|     Bob|     D102|62000.0|\n",
      "+--------+------+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stagedUpdates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fadff4d4-b6ae-4a41-b3c9-8427cb42569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a operação SCD Tipo 2 com o comando MERGE\n",
    "employeTable.alias(\"employe\").merge(\n",
    "    stagedUpdates.alias(\"staged_updates\"),\n",
    "    \"employe.emp_id = staged_updates.mergeKey\"  # A chave de mesclagem é emp_id, comparando os registros da tabela 'employe' e 'staged_updates'\n",
    ") \\\n",
    ".whenMatchedUpdate(\n",
    "    condition = \"employe.is_current = true AND (employe.salary <> staged_updates.salary OR employe.emp_name <> staged_updates.emp_name OR employe.dept_code <> staged_updates.dept_code)\", \n",
    "    set = {  \n",
    "        \"is_current\": \"false\",  # Atualiza 'is_current' para false para registros alterados\n",
    "        \"end_date\": \"current_date()\"  # Define a data de término (end_date) para o registro alterado\n",
    "    }\n",
    ") \\\n",
    ".whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"emp_id\": \"staged_updates.emp_id\",  # Insere os novos dados na tabela\n",
    "        \"emp_name\": \"staged_updates.emp_name\",\n",
    "        \"dept_code\": \"staged_updates.dept_code\",\n",
    "        \"salary\": \"staged_updates.salary\",\n",
    "        \"start_date\": \"current_date()\",  # Define a data de início (start_date) para o novo registro\n",
    "        \"end_date\": \"NULL\",  # O campo 'end_date' será NULL para novos registros\n",
    "        \"is_current\": \"true\"  # Marca o novo registro como 'atual'\n",
    "    }\n",
    ") \\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909ae842-f2f6-44dd-961d-8177128d49b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+----------+----------+----------+\n",
      "|emp_id|emp_name|dept_code| salary|start_date|  end_date|is_current|\n",
      "+------+--------+---------+-------+----------+----------+----------+\n",
      "|  1001|   Alice|     D101|58000.0|2025-03-30|      NULL|      true|\n",
      "|  1001|   Alice|     D101|50000.0|2020-01-01|2025-03-30|     false|\n",
      "|  1002|     Bob|     D102|62000.0|2025-03-30|      NULL|      true|\n",
      "|  1002|     Bob|     D102|60000.0|2020-01-01|2025-03-30|     false|\n",
      "|  1003| Charlie|     D103|70000.0|2020-01-01|      NULL|      true|\n",
      "|  1006|   Frank|     D106|70000.0|2025-03-30|      NULL|      true|\n",
      "|  1007|   Grace|     D107|75000.0|2025-03-30|      NULL|      true|\n",
      "+------+--------+---------+-------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employe_scd2 ORDER BY emp_name\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
